teacher_config:
  model_type:
    'tokenizer': BertTokenizer
    'config': BertConfig
    'model': BertTokenClassifier
    'subword_prefix': '##'
  model_checkpoint: 'cimm-kzn/enrudr-bert'
  optimizer_class: AdamW
  optimizer_kwargs:
    'lr': 1e-5
    'eps': 1e-8
    'weight_decay': 0.02
  train_batch_sz: 24
  test_batch_sz: 6
  epochs: 0
  L2_coef: 0

student_config:
  model_type:
    'tokenizer': BertTokenizer
    'config': BertConfig
    'model': BertTokenClassifier
    'subword_prefix': '##'
  model_checkpoint: 'cimm-kzn/enrudr-bert'
  optimizer_class: AdamW
  optimizer_kwargs:
    'lr': 1e-5
    'eps': 1e-8
    'weight_decay': 0.02
  train_batch_sz: 24
  test_batch_sz: 6
  epochs: 8

sampler_config:
  sampler_class: BALDSampler
  sampler_kwargs:
    'strategy': 'uncertain'
    'n_forward_passes': 8
    'scoring_batch_sz': 24
    'averaging_share': None
    #'n_samples_out': student_config.train_batch_sz
  n_samples_in: 90

exp_config:
  n_few_shot: 1000
  experiment_name: 'exp_BALD_1000_mean, mean_averaging'
  seed: 42
  teacher_set: 'joined'
  student_set: 'big'
  init_with_teacher: False
  classification_type: 'binary'
  to_sentences: True
  big_set_sample_cnt: 15000
  #common_tokenize: word_tokenize